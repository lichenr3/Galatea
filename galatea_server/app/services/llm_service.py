"""
LLM æœåŠ¡
æ— çŠ¶æ€çš„ API è°ƒç”¨å±‚ï¼Œä¸ä¿å­˜ä¼šè¯å†å²
"""
from openai import AsyncOpenAI
from app.core.config import settings
from typing import AsyncGenerator, List, Dict
from app.core.logger import get_logger

logger = get_logger(__name__)


class LLMService:
    """
    LLM API è°ƒç”¨æœåŠ¡ï¼ˆæ— çŠ¶æ€ï¼‰
    ä¸ä¿å­˜å¯¹è¯å†å²ï¼Œå†å²ç”± SessionService ç®¡ç†
    """
    
    def __init__(self):
        # æ‰“å°é…ç½®ä¿¡æ¯
        logger.info(f"ğŸ§  LLM Service åˆå§‹åŒ–:")
        logger.info(f"   - Model: {settings.LLM_MODEL}")

        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        self.client = AsyncOpenAI(
            api_key=settings.LLM_API_KEY,
            base_url=settings.LLM_BASE_URL
        )
        self.model = settings.LLM_MODEL

    async def chat_stream(
        self, 
        messages: List[Dict[str, str]],
        temperature: float = 0.7
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼å¯¹è¯ï¼ˆæ— çŠ¶æ€ï¼‰
        
        Args:
            messages: å®Œæ•´çš„æ¶ˆæ¯å†å²ï¼ˆåŒ…æ‹¬ system promptï¼‰
            temperature: æ¸©åº¦å‚æ•°ï¼ˆ0.7 é€‚åˆè§’è‰²æ‰®æ¼”ï¼‰
            
        Yields:
            LLM ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        try:
            logger.debug(f"ğŸ§  å‘é€è¯·æ±‚åˆ° LLM (æ¶ˆæ¯æ•°: {len(messages)})")
            
            # è°ƒç”¨ LLM API
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                stream=True,
                temperature=temperature
            )

            logger.debug("ğŸ§  LLM è¿æ¥å»ºç«‹ï¼Œå¼€å§‹æ¥æ”¶æ•°æ®...")

            # æµå¼è¿”å›
            async for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    yield content

            logger.debug("âœ… LLM å“åº”å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ LLM è°ƒç”¨é”™è¯¯: {e}", exc_info=True)
            yield f"[ç³»ç»Ÿé”™è¯¯: {str(e)}]"


# å•ä¾‹å¯¼å‡º
llm_service = LLMService()